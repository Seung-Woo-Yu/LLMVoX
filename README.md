# LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM

<p align="center">
<<<<<<< HEAD
    <img src="https://i.imgur.com/waxVImv.png" alt="LLMVoX">
</p>

## Authors  
**Sambal Shikar**, [Mohammed Irfan K](https://scholar.google.com/citations?user=GJp0keYAAAAJ&hl=en), [Sahal Shaji Mullappilly](https://scholar.google.com/citations?user=LJWxVpUAAAAJ&hl=en), [Fahad Khan](https://sites.google.com/view/fahadkhans/home),[Jean Lahoud](https://scholar.google.com/citations?user=LsivLPoAAAAJ&hl=en), [Rao Muhammad Anwer](https://scholar.google.com/citations?hl=en&authuser=1&user=_KlvMVoAAAAJ), [Salman Khan](https://salman-h-khan.github.io/), [Hisham Cholakkal](https://scholar.google.com/citations?hl=en&user=bZ3YBRcAAAAJ)  

### **Affiliation:**  
Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), UAE  

## Citation  
If you find our work useful, please consider citing:

```bibtex
@misc{shikhar2025llmvoxautoregressivestreamingtexttospeech,
    title={LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM},
    author={Sambhal Shikhar and Mohammed Irfan Kurpath and Sahal Shaji Mullappilly and Jean Lahoud and Fahad Khan and Rao Muhammad Anwer and Salman Khan and Hisham Cholakkal},
    year={2025},
    eprint={2503.04724},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2503.04724}
}
=======
  <img src="./assets/arch_diagram.svg" alt="LLMVoX Architecture" width="800"/>
</p>

LLMVoX is a high-performance, autoregressive streaming text-to-speech (TTS) model designed to work with any Large Language Model (LLM). It enables real-time, low-latency speech synthesis from text generated by LLMs, making it ideal for interactive conversation agents and voice assistants.

## üåü Features
- **Ultra light-weight**: Only 30M parameters
- **Streaming Architecture**: Low-latency, token-by-token text-to-speech synthesis.
- **LLM Integration**: Compatible with any text-generation LLM/Multimodal LLM.
- **Streaming inference Design**: Using a multi-queue system to generate infinite length audio.
- **Multilingual Support**: Finetune on 100+ languages
- **Fast Inference**: Optimized for real-time applications with minimal delay

## üìã Table of Contents

- [Installation](#-installation)
- [Usage](#-usage)
  - [Training](#training)
  - [Inference](#inference)
  - [API Server](#api-server)
- [Architecture](#-architecture)
- [Configuration](#-configuration)
- [Citation](#-citation)
- [License](#-license)

## üöÄ Installation

### Prerequisites

- Python 3.8+
- CUDA-compatible GPU (recommended)
- PyTorch 2.0+

### Setup

1. Clone the repository:

```bash
git clone https://github.com/yourusername/llmvox.git
cd llmvox
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Download pretrained models:

```bash
# Create a directory for model checkpoints
mkdir -p checkpoints

# Download the required model checkpoints
# WavTokenizer model
wget -O checkpoints/wavtokenizer_large_speech_320_24k.ckpt <url_to_wavtokenizer_model>

# GPT checkpoint for speech synthesis
wget -O checkpoints/ckpt_numbers.pt <url_to_speech_gpt_model>

# Text encoding model
# This will be automatically downloaded from HuggingFace
```

## üíª Usage

### Training

LLMVoX uses a GPT-based architecture for autoregressive speech token prediction. To train the model:

1. Prepare your dataset in the required JSON format:

```json
[
  {
    "speech_folder": "/path/to/audio/files",
    "speech_file": "audio1.wav",
    "ar": "Arabic text transcript",
    "id": "unique_id_1"
  },
  {
    "speech_folder": "/path/to/audio/files",
    "speech_file": "audio2.wav",
    "answer_text": "English text transcript",
    "id": "unique_id_2"
  }
]
```

2. Modify the configuration in `train_config.py` to match your training setup.

3. For single GPU training:

```bash
python train.py --batch_size=32 --compile=False
```

4. For distributed training with multiple GPUs on a single node:

```bash
torchrun --standalone --nproc_per_node=4 train.py
```

5. For distributed training across multiple nodes:

```bash
# On the master node
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=<master_ip> --master_port=1234 train.py

# On the worker node
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=<master_ip> --master_port=1234 train.py
```

### Inference

For standalone inference:

```python
from inference.model_handler import ModelHandler
from inference.llm_streaming import StreamModel
from inference_config import config
import torch

# Initialize models
model_handler = ModelHandler(config, device_id=0)
stream_model = model_handler.initialize_stream_model()

# Generate speech
system_text = "You are a helpful assistant."
prompt_text = "Tell me about artificial intelligence."

# Get text from LLM
text_streamer = stream_model.predict({"system": system_text, "prompt": prompt_text})

# Process text and generate speech
for text_chunk in text_streamer:
    # Process text to speech using model_handler
    # See the full implementation in `inference_server.py`
    pass
```

### API Server

The project includes a FastAPI server for deploying the model as a web service:

1. Start the server:

```bash
python -m uvicorn inference_server:app --host 0.0.0.0 --port 8000
```

2. Use the API:

```bash
curl -X POST "http://localhost:8000/tts" \
     -H "Content-Type: application/json" \
     -d '{"text":"What are the main challenges in artificial intelligence research today?"}'
```

The server will stream the audio response in real-time as it generates both text and speech.

## üèó Architecture

LLMVoX consists of several key components:

1. **Text Generation Model**: A streaming LLM (e.g., Llama-3) that generates text tokens one by one.

2. **Text Encoder**: A multilingual T5 model that converts text to phoneme embeddings.

3. **WavTokenizer**: A discrete audio tokenizer that converts between speech waveforms and discrete tokens.

4. **GPT Model**: An autoregressive model that predicts speech tokens based on text embeddings and previous speech tokens.

5. **Dual-Model System**: Two identical TTS models that alternate processing to maintain streaming output while the next chunk is being prepared.

<p align="center">
  <img src="https://via.placeholder.com/800x300?text=LLMVoX+Components" alt="LLMVoX Components" width="800"/>
</p>

## ‚öôÔ∏è Configuration

The system's configuration parameters are stored in `inference_config.py` and `train_config.py`. Key parameters include:

```python
# Example configuration parameters
config = {
    # Model architecture
    "n_layer": 4,
    "n_head": 8,
    "n_embd": 768,
    "block_size": 8192,
    
    # Model paths
    "encoder_model_path": "charsiu/g2p_multilingual_byT5_tiny_16_layers_100",
    "wav_model_path": "checkpoints/wavtokenizer_large_speech_320_24k.ckpt",
    
    # Inference settings
    "tts_device_1": 0,
    "tts_device_2": 0,
    "initial_dump_size_1": 8,
    "initial_dump_size_2": 8,
    "max_dump_size": 32,
    "max_audio_length": 8192,
    
    # API settings
    "api_host": "0.0.0.0",
    "api_port": 8000,
    
    # Special tokens
    "eos_token": "[EOS]",
    "pad_token_id": 384,
    "eoa_token_id": 453,
}
```

## üìù Citation

If you use LLMVoX in your research, please cite our paper:

```bibtex
@article{your-paper-id,
  title={LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM},
  author={Your Name},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2023}
}
```

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.
>>>>>>> Initial commit
